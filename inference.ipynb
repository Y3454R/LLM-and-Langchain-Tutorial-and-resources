{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'alpaca-lora'...\n",
      "remote: Enumerating objects: 607, done.\u001b[K\n",
      "remote: Total 607 (delta 0), reused 0 (delta 0), pack-reused 607\u001b[K\n",
      "Receiving objects: 100% (607/607), 27.84 MiB | 8.89 MiB/s, done.\n",
      "Resolving deltas: 100% (357/357), done.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/tloen/alpaca-lora.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "import transformers\n",
    "import textwrap\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "from transformers.generation.utils import GreedySearchDecoderOnlyOutput\n",
    " \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:06<00:00,  5.14it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    " \n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"decapoda-research/llama-7b-hf\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, \"tloen/alpaca-lora-7b\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2\n",
    " \n",
    "model = model.eval()\n",
    "# model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = f\"\"\"\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    " \n",
    "### Instruction:\n",
    "[INSTRUCTION]\n",
    " \n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      " \n",
      "### Instruction:\n",
      "What is the meaning of life?\n",
      " \n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_prompt(instruction: str) -> str:\n",
    "    return PROMPT_TEMPLATE.replace(\"[INSTRUCTION]\", instruction)\n",
    " \n",
    "print(create_prompt(\"What is the meaning of life?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, model: PeftModel) -> GreedySearchDecoderOnlyOutput:\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"].to(DEVICE)\n",
    " \n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=0.75,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    with torch.inference_mode():\n",
    "        return model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=256,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(response: GreedySearchDecoderOnlyOutput) -> str:\n",
    "    decoded_output = tokenizer.decode(response.sequences[0])\n",
    "    response = decoded_output.split(\"### Response:\")[1].strip()\n",
    "    return \"\\n\".join(textwrap.wrap(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_alpaca(prompt: str, model: PeftModel = model) -> str:\n",
    "    prompt = create_prompt(prompt)\n",
    "    response = generate_response(prompt, model)\n",
    "    print(format_response(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is to find joy and purpose in every moment,\n",
      "regardless of external circumstances. It is to live with intention and\n",
      "make the most out of each day.\n"
     ]
    }
   ],
   "source": [
    "ask_alpaca(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13bf4f41b44d569fb42cda98089b9a8023fba74126e96c54c8f2e63f2d74dfb7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('hugging1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
