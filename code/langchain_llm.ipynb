{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653b1c8185444c0e95ca6e1c2dee21ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r'/home/drmohammad/Documents/LLM/Llamav2hf/Llama-2-7b-chat-hf'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"daryl149/llama-2-7b-chat-hf\",\n",
    "                                          use_auth_token=True,)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"daryl149/llama-2-7b-chat-hf\",\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             use_auth_token=True,\n",
    "                                            #  load_in_8bit=True,\n",
    "                                            #  load_in_4bit=True\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.80it/s]\n"
     ]
    }
   ],
   "source": [
    "path=r'/home/drmohammad/Documents/LLM/Llamav2hf/Llama-2-7b-chat-hf'\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(path,\n",
    "                                          use_auth_token=True,)\n",
    "\n",
    "model2 = AutoModelForCausalLM.from_pretrained(path,\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             use_auth_token=True,\n",
    "                                            #  load_in_8bit=True,\n",
    "                                            #  load_in_4bit=True\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline for later\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 512,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "\n",
    "def cut_off_text(text, prompt):\n",
    "    cutoff_phrase = prompt\n",
    "    index = text.find(cutoff_phrase)\n",
    "    if index != -1:\n",
    "        return text[:index]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_substring(string, substring):\n",
    "    return string.replace(substring, \"\")\n",
    "\n",
    "\n",
    "\n",
    "def generate(text):\n",
    "    prompt = get_prompt(text)\n",
    "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=512,\n",
    "                                 eos_token_id=tokenizer.eos_token_id,\n",
    "                                 pad_token_id=tokenizer.eos_token_id,\n",
    "                                 )\n",
    "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        final_outputs = cut_off_text(final_outputs, '</s>')\n",
    "        final_outputs = remove_substring(final_outputs, prompt)\n",
    "\n",
    "    return final_outputs#, outputs\n",
    "\n",
    "def parse_text(text):\n",
    "        wrapped_text = textwrap.fill(text, width=100)\n",
    "        print(wrapped_text +'\\n\\n')\n",
    "        # return assistant_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\nWhat is the temperature in Melbourne?[/INST]\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"What is the temperature in Melbourne?\"\n",
    "\n",
    "get_prompt(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST]<<SYS>>\\nYou are an expert and summarization and reducing the number of words used\\n<</SYS>>\\n\\nSummarize the following text for me {text}[/INST]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "instruction = \"Summarize the following text for me {text}\"\n",
    "\n",
    "system_prompt = \"You are an expert and summarization and reducing the number of words used\"\n",
    "\n",
    "get_prompt(instruction, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate,  LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "You are an advanced assistant that excels at translation. \n",
      "<</SYS>>\n",
      "\n",
      "Convert the following text from English to French:\n",
      "\n",
      " {text}[/INST]\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are an advanced assistant that excels at translation. \"\n",
    "instruction = \"Convert the following text from English to French:\\n\\n {text}\"\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here is the translation of \"how are you today?\" from English to French:  Comment: Comment\n",
      "vas-tu aujourd'hui?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"how are you today?\"\n",
    "output = llm_chain.run(text)\n",
    "\n",
    "parse_text(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "You are an expert and summarization and expressing key ideas succintly\n",
      "<</SYS>>\n",
      "\n",
      "Summarize the following article for me {text}[/INST]\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Summarize the following article for me {text}\"\n",
    "system_prompt = \"You are an expert and summarization and expressing key ideas succintly\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "940"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_words(input_string):\n",
    "    words = input_string.split(\" \")\n",
    "    return len(words)\n",
    "\n",
    "text = '''Twitter (now X) CEO Linda Yaccarino claims usage at ‘all time high’ in memo to staff\n",
    "Twitter’s (now X’s) newly established CEO Linda Yaccarino touts the company’s success and X’s future plans in a company-wide memo obtained by CNBC. The exec once again claims, without sharing any specific metrics, that the service’s usage is at an “all time high,” and hints at what’s to come in terms of new product experiences for the newly rebranded platform.\n",
    "\n",
    "The service formerly known as Twitter has been working to become more than just a social network and more of an “everything app,” as owner Elon Musk dubbed it.\n",
    "\n",
    "As the Telsa and Space X exec explained in October 2022, telegraphing Twitter’s eventual rebranding, buying Twitter was meant to be “an accelerant to creating X, the everything app.”\n",
    "\n",
    "\n",
    "His grand plan has been to create an app that allows creators to monetize their content, then later moves into payments services and even banking, Musk remarked during a Twitter Spaces livestream with advertisers in November. At the time, he even mentioned the possibility of establishing money market accounts on Twitter that would pay a high-interest rate to attract consumers to X.\n",
    "\n",
    "Those possible product concepts were again referenced in Yaccarino’s new missive, when she writes, “Our usage is at an all time high and we’ll continue to delight our entire community with new experiences in audio, video, messaging, payments, banking – creating a global marketplace for ideas, goods, services, and opportunities.”\n",
    "\n",
    "Twitter, now X, has already implemented some of Musk’s ideas around videos and creator monetization. In May, the company began allowing subscribers to upload two-hour videos to its service, which advertiser Apple then leveraged when it released the entire first episode of its hit Apple TV+ show “Silo” on the platform. Fired Fox News host Tucker Carlson had been posting lengthy videos to Twitter as well, until ordered to stop by the network.\n",
    "\n",
    "In addition, earlier this month, Twitter began sharing ad revenue with verified creators.\n",
    "\n",
    "However, all is not well at Twitter X, whose traffic — at least by third-party measurements — has been dropping. Data from web analytics firm Similarweb indicated Twitter’s web traffic declined 5% for the first two days its latest rival, Instagram Threads, became generally available, compared with the week prior. Plus, Similarweb said Twitter’s web traffic was down 11% compared with the same days in 2022. Additionally, Cloudflare CEO Matthew Prince earlier this month tweeted a graph of traffic to the Twitter.com domain that showed “Twitter traffic tanking,” he said.\n",
    "\n",
    "\n",
    "Yaccarino subtly pushed back at those reports at the time, claiming that Twitter had its largest usage day since February in early July. She did not share any specific metrics or data. At the same time, however, the company was quietly blocking links to Threads.net in Twitter searches, suggesting it was concerned about the new competition.\n",
    "\n",
    "Today, Yaccarino repeats her vague claims around X’s high usage in her company-wide memo even as analysts at Forrester are predicting X will either shut down or be acquired within the next 12 months and numerous critics concur that the X rebrand is destined to fail.\n",
    "\n",
    "Yaccarino’s memo, otherwise, was mostly a lot of cheerleading, applauding X’s team for their work and touting X’s ability to “impress the world all over again,” as Twitter once did.\n",
    "\n",
    "The full memo, courtesy of CBNC, is below:\n",
    "\n",
    "Hi team,\n",
    "\n",
    "What a momentous weekend. As I said yesterday, it’s extremely rare, whether it’s in life or in business, that you have the opportunity to make another big impression. That’s what we’re experiencing together, in real time. Take a moment to put it all into perspective.\n",
    "\n",
    "17 years ago, Twitter made a lasting imprint on the world. The platform changed the speed at which people accessed information. It created a new dynamic for how people communicated, debated, and responded to things happening in the world. Twitter introduced a new way for people, public figures, and brands to build long lasting relationships. In one way or another, everyone here is a driving force in that change. But equally all our users and partners constantly challenged us to dream bigger, to innovate faster, and to fulfill our great potential.\n",
    "\n",
    "With X we will go even further to transform the global town square — and impress the world all over again.\n",
    "\n",
    "Our company uniquely has the drive to make this possible. Many companies say they want to move fast — but we enjoy moving at the speed of light, and when we do, that’s X. At our core, we have an inventor mindset — constantly learning, testing out new approaches, changing to get it right and ultimately succeeding.\n",
    "\n",
    "With X, we serve our entire community of users and customers by working tirelessly to preserve free expression and choice, create limitless interactivity, and create a marketplace that enables the economic success of all its participants.\n",
    "\n",
    "The best news is we’re well underway. Everyone should be proud of the pace of innovation over the last nine months — from long form content, to creator monetization, and tremendous advancements in brand safety protections. Our usage is at an all time high and we’ll continue to delight our entire community with new experiences in audio, video, messaging, payments, banking – creating a global marketplace for ideas, goods, services, and opportunities.\n",
    "\n",
    "Please don’t take this moment for granted. You’re writing history, and there’s no limit to our transformation. And everyone, is invited to build X with us.\n",
    "\n",
    "Elon and I will be working across every team and partner to bring X to the world. That includes keeping our entire community up to date, ensuring that we all have the information we need to move forward.\n",
    "\n",
    "Now, let’s go make that next big impression on the world, together.\n",
    "\n",
    "Linda'''\n",
    "\n",
    "count_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "  Linda Yaccarino, the newly appointed CEO of Twitter (now rebranded as X), has issued a company-\n",
      "wide memo to express her excitement and optimism about the platform's current success and future\n",
      "plans. Despite recent reports of traffic decline, Yaccarino claims that usage is at an all-time high\n",
      "and will continue to grow with new product experiences in audio, video, messaging, payments, and\n",
      "banking. She emphasizes the platform's goal of creating a global marketplace for ideas, goods,\n",
      "services, and opportunities, and encourages the team to keep pushing boundaries and innovating.\n",
      "Yaccarino also mentions Elon Musk's vision for X as an \"everything app,\" and how the company will\n",
      "work together to bring it to the world.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = llm_chain.run(text)\n",
    "print(count_words(output))\n",
    "parse_text(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import LLMChain, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
      "<</SYS>>\n",
      "\n",
      "Chat History:\n",
      "\n",
      "{chat_history} \n",
      "\n",
      "User: {user_input}[/INST]\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Chat History:\\n\\n{chat_history} \\n\\nUser: {user_input}\"\n",
    "system_prompt = \"You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"user_input\"], template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
      "<</SYS>>\n",
      "\n",
      "Chat History:\n",
      "\n",
      " \n",
      "\n",
      "User: Can you tell me about yourself.[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"  Of course! I'm just an AI designed to assist and provide helpful responses. I'm here to help you with any questions or topics you'd like to discuss. Is there something specific you'd like to know or talk about?\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(user_input=\"Can you tell me about yourself.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
      "<</SYS>>\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: Can you tell me about yourself.\n",
      "AI:   Of course! I'm just an AI designed to assist and provide helpful responses. I'm here to help you with any questions or topics you'd like to discuss. Is there something specific you'd like to know or talk about? \n",
      "\n",
      "User: Today is Friday. What number day of the week is that?[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  AI: Today is the fifth day of the week.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(user_input=\"Today is Friday. What number day of the week is that?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
      "<</SYS>>\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: Can you tell me about yourself.\n",
      "AI:   Of course! I'm just an AI designed to assist and provide helpful responses. I'm here to help you with any questions or topics you'd like to discuss. Is there something specific you'd like to know or talk about?\n",
      "Human: Today is Friday. What number day of the week is that?\n",
      "AI:   AI: Today is the fifth day of the week. \n",
      "\n",
      "User: what is the day today?[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"  Hello! I'm happy to help you with your query. Today is Friday.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(user_input=\"what is the day today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
      "<</SYS>>\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: Can you tell me about yourself.\n",
      "AI:   Of course! I'm just an AI designed to assist and provide helpful responses. I'm here to help you with any questions or topics you'd like to discuss. Is there something specific you'd like to know or talk about?\n",
      "Human: Today is Friday. What number day of the week is that?\n",
      "AI:   AI: Today is the fifth day of the week.\n",
      "Human: what is the day today?\n",
      "AI:   Hello! I'm happy to help you with your query. Today is Friday. \n",
      "\n",
      "User: What is my name?[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"  Hello! I'm just an AI, and I'm here to help you with any questions or tasks you may have. However, I cannot provide personal information such as your name or any other personal details without your explicit consent. It's important to respect people's privacy and security online. Is there anything else I can help you with?\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(user_input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
      "<</SYS>>\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: Can you tell me about yourself.\n",
      "AI:   Of course! I'm just an AI designed to assist and provide helpful responses. I'm here to help you with any questions or topics you'd like to discuss. Is there something specific you'd like to know or talk about?\n",
      "Human: Today is Friday. What number day of the week is that?\n",
      "AI:   AI: Today is the fifth day of the week.\n",
      "Human: what is the day today?\n",
      "AI:   Hello! I'm happy to help you with your query. Today is Friday.\n",
      "Human: What is my name?\n",
      "AI:   Hello! I'm just an AI, and I'm here to help you with any questions or tasks you may have. However, I cannot provide personal information such as your name or any other personal details without your explicit consent. It's important to respect people's privacy and security online. Is there anything else I can help you with? \n",
      "\n",
      "User: Can you tell me about the olympics[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"  Of course! I'm happy to help you with information about the Olympics. The Olympics are an international multi-sport event held every four years, where athletes from around the world compete in various sports such as track and field, gymnastics, swimming, and many more. The Olympic Games have a rich history dating back to ancient Greece, and they continue to be a symbol of unity and athletic excellence. Is there something specific you would like to know about the Olympics?\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(user_input=\"Can you tell me about the olympics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
      "<</SYS>>\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: Can you tell me about yourself.\n",
      "AI:   Of course! I'm just an AI designed to assist and provide helpful responses. I'm here to help you with any questions or topics you'd like to discuss. Is there something specific you'd like to know or talk about?\n",
      "Human: Today is Friday. What number day of the week is that?\n",
      "AI:   AI: Today is the fifth day of the week.\n",
      "Human: what is the day today?\n",
      "AI:   Hello! I'm happy to help you with your query. Today is Friday.\n",
      "Human: What is my name?\n",
      "AI:   Hello! I'm just an AI, and I'm here to help you with any questions or tasks you may have. However, I cannot provide personal information such as your name or any other personal details without your explicit consent. It's important to respect people's privacy and security online. Is there anything else I can help you with?\n",
      "Human: Can you tell me about the olympics\n",
      "AI:   Of course! I'm happy to help you with information about the Olympics. The Olympics are an international multi-sport event held every four years, where athletes from around the world compete in various sports such as track and field, gymnastics, swimming, and many more. The Olympic Games have a rich history dating back to ancient Greece, and they continue to be a symbol of unity and athletic excellence. Is there something specific you would like to know about the Olympics? \n",
      "\n",
      "User: What have we talked about in this Chat?[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  In this chat, you and I have discussed the following topics:\\n1. You asked me about myself, and I provided a brief introduction as an AI assistant designed to assist and provide helpful responses.\\n2. You asked me the day of the week today, and I replied that it is Friday.\\n3. You asked me your name, and I explained that I cannot provide personal information without your explicit consent.\\n4. You asked me about the Olympics, and I provided a brief overview of the event, including its history and the various sports involved.\\nPlease let me know if there is anything else you would like to discuss or learn about!'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(user_input=\"What have we talked about in this Chat?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13bf4f41b44d569fb42cda98089b9a8023fba74126e96c54c8f2e63f2d74dfb7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('hugging1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
