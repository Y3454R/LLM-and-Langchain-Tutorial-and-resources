{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import textwrap\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    " \n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    ")\n",
    " \n",
    "import fire\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    " \n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(10, 7)})\n",
    "sns.set(rc={'figure.dpi':100})\n",
    "sns.set(style='white', palette='muted', font_scale=1.2)\n",
    " \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fri Mar 23 00:40:40 +0000 2018</td>\n",
       "      <td>@p0nd3ea Bitcoin wasn't built to live on excha...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fri Mar 23 00:40:40 +0000 2018</td>\n",
       "      <td>@historyinflicks Buddy if I had whatever serie...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Mar 23 00:40:42 +0000 2018</td>\n",
       "      <td>@eatBCH @Bitcoin @signalapp @myWickr @Samsung ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fri Mar 23 00:41:04 +0000 2018</td>\n",
       "      <td>@aantonop Even if Bitcoin crash tomorrow morni...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fri Mar 23 00:41:07 +0000 2018</td>\n",
       "      <td>I am experimenting whether I can live only wit...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             date  \\\n",
       "0  Fri Mar 23 00:40:40 +0000 2018   \n",
       "1  Fri Mar 23 00:40:40 +0000 2018   \n",
       "2  Fri Mar 23 00:40:42 +0000 2018   \n",
       "3  Fri Mar 23 00:41:04 +0000 2018   \n",
       "4  Fri Mar 23 00:41:07 +0000 2018   \n",
       "\n",
       "                                               tweet  sentiment  \n",
       "0  @p0nd3ea Bitcoin wasn't built to live on excha...        1.0  \n",
       "1  @historyinflicks Buddy if I had whatever serie...        1.0  \n",
       "2  @eatBCH @Bitcoin @signalapp @myWickr @Samsung ...        0.0  \n",
       "3  @aantonop Even if Bitcoin crash tomorrow morni...        0.0  \n",
       "4  I am experimenting whether I can live only wit...        1.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"bitcoin-sentiment-tweets.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Detect the sentiment of the tweet.',\n",
       " 'input': \"@p0nd3ea Bitcoin wasn't built to live on exchanges.\",\n",
       " 'output': 'Positive'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentiment_score_to_name(score: float):\n",
    "    if score > 0:\n",
    "        return \"Positive\"\n",
    "    elif score < 0:\n",
    "        return \"Negative\"\n",
    "    return \"Neutral\"\n",
    " \n",
    "dataset_data = [\n",
    "    {\n",
    "        \"instruction\": \"Detect the sentiment of the tweet.\",\n",
    "        \"input\": row_dict[\"tweet\"],\n",
    "        \"output\": sentiment_score_to_name(row_dict[\"sentiment\"])\n",
    "    }\n",
    "    for row_dict in df.to_dict(orient=\"records\")\n",
    "]\n",
    " \n",
    "dataset_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"alpaca-bitcoin-sentiment-dataset.json\", \"w\") as f:\n",
    "   json.dump(dataset_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:07<00:00,  4.49it/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL = \"decapoda-research/llama-7b-hf\"\n",
    " \n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    " \n",
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    " \n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/drmohammad/.cache/huggingface/datasets/json/default-c27f2983a0808b40/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 2158.67it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1324.38it/s]\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/drmohammad/.cache/huggingface/datasets/json/default-c27f2983a0808b40/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1091.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output'],\n",
       "    num_rows: 1897\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files=\"alpaca-bitcoin-sentiment-dataset.json\")\n",
    "data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    " \n",
    "CUTOFF_LEN=256\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < CUTOFF_LEN\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    " \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    " \n",
    "    return result\n",
    " \n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/drmohammad/.cache/huggingface/datasets/json/default-c27f2983a0808b40/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-780d335a0130d2fd.arrow and /home/drmohammad/.cache/huggingface/datasets/json/default-c27f2983a0808b40/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eb3203240a371a4a.arrow\n",
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=200, shuffle=True, seed=42\n",
    ")\n",
    "train_data = (\n",
    "    train_val[\"train\"].map(generate_and_tokenize_prompt)\n",
    ")\n",
    "val_data = (\n",
    "    train_val[\"test\"].map(generate_and_tokenize_prompt)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT= 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    " \n",
    "BATCH_SIZE = 128\n",
    "MICRO_BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LEARNING_RATE = 3e-4\n",
    "TRAIN_STEPS = 300\n",
    "OUTPUT_DIR = \"experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_int8_training(model)\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_steps=100,\n",
    "    max_steps=TRAIN_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]/home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  3%|▎         | 10/300 [05:56<2:52:00, 35.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2303, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 11/300 [06:31<2:50:18, 35.36s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 18\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mstate_dict \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m__: get_peft_model_state_dict(\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28mself\u001b[39m, old_state_dict()\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m )\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(model, \u001b[38;5;28mtype\u001b[39m(model))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# model = torch.compile(model)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(OUTPUT_DIR)\n",
      "File \u001b[0;32m~/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1639'>1640</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1641'>1642</a>\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1642'>1643</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1643'>1644</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1644'>1645</a>\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1645'>1646</a>\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1646'>1647</a>\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1647'>1648</a>\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1648'>1649</a>\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1649'>1650</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1934'>1935</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1936'>1937</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1937'>1938</a>\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1939'>1940</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1940'>1941</a>\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1941'>1942</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1942'>1943</a>\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1943'>1944</a>\u001b[0m ):\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1944'>1945</a>\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=1945'>1946</a>\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py:2770\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=2767'>2768</a>\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=2768'>2769</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=2769'>2770</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/transformers/trainer.py?line=2771'>2772</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/anaconda3/envs/hugging1/lib/python3.9/site-packages/accelerate/accelerator.py:1819\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/accelerate/accelerator.py?line=1816'>1817</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/accelerate/accelerator.py?line=1817'>1818</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/accelerate/accelerator.py?line=1818'>1819</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/accelerate/accelerator.py?line=1819'>1820</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/accelerate/accelerator.py?line=1820'>1821</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/hugging1/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/torch/_tensor.py?line=477'>478</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/torch/_tensor.py?line=478'>479</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/torch/_tensor.py?line=479'>480</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/torch/_tensor.py?line=480'>481</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/torch/_tensor.py?line=485'>486</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/torch/_tensor.py?line=486'>487</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/torch/_tensor.py?line=487'>488</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/torch/_tensor.py?line=488'>489</a>\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/torch/_tensor.py?line=489'>490</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/hugging1/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/torch/autograd/__init__.py?line=191'>192</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/torch/autograd/__init__.py?line=193'>194</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/torch/autograd/__init__.py?line=194'>195</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/torch/autograd/__init__.py?line=195'>196</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/torch/autograd/__init__.py?line=196'>197</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/torch/autograd/__init__.py?line=197'>198</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///home/drmohammad/anaconda3/envs/hugging1/lib/python3.9/site-packages/torch/autograd/__init__.py?line=198'>199</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "model.config.use_cache = False\n",
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(\n",
    "        self, old_state_dict()\n",
    "    )\n",
    ").__get__(model, type(model))\n",
    " \n",
    "# model = torch.compile(model)\n",
    " \n",
    "trainer.train()\n",
    "model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13bf4f41b44d569fb42cda98089b9a8023fba74126e96c54c8f2e63f2d74dfb7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('hugging1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
