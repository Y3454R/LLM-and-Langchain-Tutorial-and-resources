{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rtx3090/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from trl import SFTTrainer\n",
    "import transformers\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import GenerationConfig\n",
    "from pynvml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "lora_output = 'KUETLLM/KUETLLM_zephyr7b_lora'\n",
    "full_output = 'KUETLLM/KUETLLM_zephyr7b_beta'\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login(\"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\") #arbit\n",
    "# login(\"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\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### huggingface dataset with Prompt, Answer pair\n",
    "\n",
    "# data = load_dataset(\"huggingface/repo\", split=\"train\")\n",
    "# data_df = data.to_pandas()\n",
    "\n",
    "### read csv with Prompt, Answer pair \n",
    "data_location = r\"dataset_shakib.xlsx\"\n",
    "# data_df=pd.read_csv( data_location ,encoding='unicode_escape')\n",
    "data_df=pd.read_excel( data_location)\n",
    "\n",
    "### formatting function using tokenizer chat template, system text is set for KUETLLM\n",
    "# def formatted_text(x):\n",
    "#     temp = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a KUET authority managed chatbot, help users by answering their queries about KUET.\"},\n",
    "#     {\"role\": \"user\", \"content\": x[\"Prompt\"]},\n",
    "#     {\"role\": \"assistant\", \"content\": x[\"Reply\"]}\n",
    "#     ]\n",
    "#     return tokenizer.apply_chat_template(temp, add_generation_prompt=False, tokenize=False)\n",
    "\n",
    "\n",
    "### set formatting\n",
    "# data_df[\"text\"] = data_df[[\"Prompt\", \"Reply\"]].apply(lambda x: formatted_text(x), axis=1)\n",
    "# print(data_df.iloc[0])\n",
    "# dataset = Dataset.from_pandas(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt           What is the history and evolution of KUET?\n",
      "Reply     Khulna University of Engineering & Technology ...\n",
      "text      <s>### Instruction\\n    Use the provided input...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def formatted_text(x):\n",
    "    return f'''<s>### Instruction\n",
    "    Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
    "\n",
    "    ### Input:\n",
    "    {x[\"Prompt\"]}\n",
    "\n",
    "    ### Response:\n",
    "    {x[\"Reply\"]}</s>\n",
    "    '''\n",
    "data_df[\"text\"] = data_df[[\"Prompt\", \"Reply\"]].apply(lambda x: formatted_text(x), axis=1)\n",
    "print(data_df.iloc[0])\n",
    "dataset = Dataset.from_pandas(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>### Instruction\n",
      "    Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
      "\n",
      "    ### Input:\n",
      "    What is the history and evolution of KUET?\n",
      "\n",
      "    ### Response:\n",
      "    Khulna University of Engineering & Technology (KUET) originated as Khulna Engineering College in 1974. It transformed into the Bangladesh Institute of Technology (BIT), Khulna in 1986. Later, in 2003, it was upgraded and renamed as Khulna University of Engineering & Technology with the aim of enhancing higher education and research in engineering and technology. The campus spans 101 acres, offering a picturesque setting for academic pursuits. KUET aspires to be a leading engineering university with a focus on teaching, research, and intellectual development.</s>\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(data_df.iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363b7926159e44918731f512467db5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Get quantized model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                                          load_in_8bit=True,     # call for the 8 bit bnb quantized version\n",
    "                                                          device_map='auto'\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print model to find lora layers\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PEFT adapter config (16:32)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# target modules are currently selected for zephyr base model\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\",\"k_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],   # target all the linear layers for full finetuning\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stabilize output layer and layernorms\n",
    "model = prepare_model_for_kbit_training(model, 8)\n",
    "# Set PEFT adapter on model (Last step)\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters\n",
    "MAXLEN=512\n",
    "BATCH_SIZE=4\n",
    "GRAD_ACC=4\n",
    "OPTIMIZER='paged_adamw_8bit' # save memory\n",
    "LR=5e-06                      # slightly smaller than pretraining lr | and close to LoRA standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rtx3090/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:194: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c04ec8adcd4fa2a987f2e9e31399e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rtx3090/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:247: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set training config\n",
    "training_config = transformers.TrainingArguments(per_device_train_batch_size=BATCH_SIZE,\n",
    "                                                 gradient_accumulation_steps=GRAD_ACC,\n",
    "                                                 optim=OPTIMIZER,\n",
    "                                                 learning_rate=LR,\n",
    "                                                 fp16=True,            # consider compatibility when using bf16\n",
    "                                                 logging_steps=10,\n",
    "                                                 num_train_epochs = 2,\n",
    "                                                 output_dir=lora_output,\n",
    "                                                 remove_unused_columns=False,\n",
    "                                                 )\n",
    "\n",
    "# Set collator\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "# Setup trainer\n",
    "trainer = SFTTrainer(model=model,\n",
    "                               train_dataset=dataset,\n",
    "                               data_collator=data_collator,\n",
    "                               args=training_config,\n",
    "                               dataset_text_field=\"text\",\n",
    "                            #    callbacks=[early_stop], need to learn, lora easily overfits\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f5724cf0874229b3025cb962c90055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/rtx3090/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/rtx3090/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3995, 'learning_rate': 3.6764705882352946e-06, 'epoch': 0.56}\n",
      "{'loss': 2.0903, 'learning_rate': 2.2058823529411767e-06, 'epoch': 1.13}\n",
      "{'loss': 1.9241, 'learning_rate': 7.352941176470589e-07, 'epoch': 1.69}\n",
      "{'train_runtime': 278.9817, 'train_samples_per_second': 2.022, 'train_steps_per_second': 0.122, 'train_loss': 2.1151882339926327, 'epoch': 1.92}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=34, training_loss=2.1151882339926327, metrics={'train_runtime': 278.9817, 'train_samples_per_second': 2.022, 'train_steps_per_second': 0.122, 'train_loss': 2.1151882339926327, 'epoch': 1.92})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(lora_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get peft config\n",
    "from peft import PeftConfig\n",
    "config = PeftConfig.from_pretrained(lora_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6adcda27e0a24ccab831b2b07feebe33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get base model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
    "                                                          return_dict=True,\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('zephyr-7b-beta-base-full/tokenizer_config.json',\n",
       " 'zephyr-7b-beta-base-full/special_tokens_map.json',\n",
       " 'zephyr-7b-beta-base-full/tokenizer.model',\n",
       " 'zephyr-7b-beta-base-full/added_tokens.json',\n",
       " 'zephyr-7b-beta-base-full/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(base_model,\n",
    "                                                       add_eos_token=True\n",
    "                                                       )\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.save_pretrained(\"zephyr-7b-beta-base-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Lora model\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, lora_output)\n",
    "\n",
    "# Get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(config.base_model_name_or_path,\n",
    "                                                       add_eos_token=True\n",
    "                                                       )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('KUETLLM/KUETLLM_zephyr7b_beta/tokenizer_config.json',\n",
       " 'KUETLLM/KUETLLM_zephyr7b_beta/special_tokens_map.json',\n",
       " 'KUETLLM/KUETLLM_zephyr7b_beta/tokenizer.model',\n",
       " 'KUETLLM/KUETLLM_zephyr7b_beta/added_tokens.json',\n",
       " 'KUETLLM/KUETLLM_zephyr7b_beta/tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.save_pretrained(full_output)\n",
    "tokenizer.save_pretrained(full_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # push model to hub\n",
    "# merged_model.push_to_hub(full_output)\n",
    "# tokenizer.push_to_hub(full_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252525ab89284bd7b005519bc5eb8603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load for inferencing\n",
    "tokenizer = AutoTokenizer.from_pretrained(full_output)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(full_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load base for comparison\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"zephyr7b-beta-full\")\n",
    "# tokenizer.save_pretrained(\"zephyr7b-beta-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### inferencing on the new model\n",
    "\n",
    "def process_data_sample(example):\n",
    "\n",
    "    processed_example = \"<|system|>\\n You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\\n<|user|>\\n\" + example[\"instruction\"] + \"\\n<|assistant|>\\n\"\n",
    "\n",
    "    return processed_example\n",
    "\n",
    "inp_str = process_data_sample(\n",
    "    {\n",
    "        # \"instruction\": \"Tell me about the importance of Tawaf.\",\n",
    "        # \"instruction\": \"What are the steps of Tawaf?\",\n",
    "        # \"instruction\": \"What are the steps of Hajj?\",\n",
    "        # \"instruction\": \"What happens in Mina during Hajj?\",\n",
    "        \"instruction\": \"Can you describe the location of Khulna City?\",\n",
    "        # \"instruction\": \"Tell me about the history of Hajj.\",\n",
    "        # \"instruction\": \"What started Hajj historically?\",\n",
    "        # \"instruction\": \"what is the origin of hajj?\",\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_text(x):\n",
    "    return f'''<s>### Instruction\n",
    "    Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
    "\n",
    "    ### Input:\n",
    "    {x[\"Prompt\"]}\n",
    "\n",
    "    ### Response:\n",
    "    {x[\"Reply\"]}</s>\n",
    "    '''\n",
    "inp=f'''<s>### Instruction\n",
    "    Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
    "\n",
    "    ### Input:\n",
    "    Can you describe the location of Khulna City?\n",
    "\n",
    "    ### Response:\n",
    "    </s>\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"\", return_tensors=\"pt\")\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    top_k=1,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "import time\n",
    "st_time = time.time()\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(time.time()-st_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetuned:\n",
    "# <|system|>\n",
    "#  You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n",
    "# <|user|>\n",
    "# What started Hajj historically?\n",
    "# <|assistant|>\n",
    "# Hajj started as a pilgrimage to the Kaaba, which was a sacred site for the ancient Arab tribes. The Kaaba was believed to be the house of Allah, and the pilgrimage was a way to honor and worship Him. The practice of Hajj became more formalized and structured over time, with specific rituals and traditions developing. Today, Hajj is a significant religious event for Muslims around the world.\n",
    "# 108.65042996406555\n",
    "\n",
    "# <|system|>\n",
    "#  You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n",
    "# <|user|>\n",
    "# Tell me about the history of Hajj.\n",
    "# <|assistant|>\n",
    "# Hajj is one of the five pillars of Islam, and its history dates back to the time of the Prophet Muhammad. The first recorded Hajj was in 632 CE, during the Prophet's lifetime. The rituals of Hajj have evolved over time, with some changes made by the Prophet himself and others added by subsequent generations of Muslims. The Hajj pilgrimage has also played a significant role in Islamic history, serving as a symbol of unity and solidarity among Muslims from different parts of the world. Today, Hajj remains a central part of Islamic practice, attracting millions of pilgrims each year.\n",
    "# 159.5786316394806\n",
    "\n",
    "# <|system|>\n",
    "#  You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n",
    "# <|user|>\n",
    "# what is the origin of hajj?\n",
    "# <|assistant|>\n",
    "# Hajj is a religious pilgrimage to Mecca, Saudi Arabia, that is an essential part of Islamic faith. The practice of hajj dates back to the time of the Prophet Muhammad, who established it as a religious obligation for Muslims. The pilgrimage is a symbolic reenactment of the experiences of the Prophet Muhammad and his wife, Khadijah, during their pilgrimage to Mecca. The practice of hajj has been an integral part of Islamic tradition for over 1,400 years.\n",
    "# 134.81562113761902\n",
    "\n",
    "# Base:\n",
    "# <|system|>\n",
    "#  You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n",
    "# <|user|>\n",
    "# What started Hajj historically?\n",
    "# <|assistant|>\n",
    "# The practice of Hajj, as one of the five pillars of Islam, can be traced back to the time of Prophet Muhammad (peace be upon him) in the 7th century CE. However, the concept of pilgrimage to the holy city of Mecca for worship and devotion can be traced back to pre-Islamic times, as evidenced by historical records and archaeological findings. The ancient Arab tribes used to visit the Kaaba, a cube-shaped structure in the center of the Grand Mosque in Mecca, as a place of worship and pilgrimage. The Prophet Muhammad's mission to unify and spread Islam brought a new dimension to the practice of Hajj, making it a mandatory religious obligation for all able-bodied Muslims who can afford it to perform once in their lifetime.\n",
    "# 208.76106905937195\n",
    "\n",
    "# <|system|>\n",
    "#  You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n",
    "# <|user|>\n",
    "# Tell me about the history of Hajj.\n",
    "# <|assistant|>\n",
    "# Certainly! The practice of Hajj, which is one of the five pillars of Islam, has a rich and fascinating history that dates back over 1,400 years.\n",
    "\n",
    "# The origins of Hajj can be traced back to the time of the Prophet Muhammad, who received the first revelation of the Quran in 610 CE. According to Islamic tradition, the Prophet Muhammad was commanded by God to make a pilgrimage to the holy city of Mecca, which at the time was a pagan center of worship.\n",
    "\n",
    "# The Prophet Muhammad completed his first pilgrimage, known as the Farewell Pilgrimage, in 632 CE, just a few months before his death. This pilgrimage is considered to be the first official Hajj, and it established many of the rituals and traditions that are still observed today.\n",
    "\n",
    "# Over the centuries, Hajj has played a significant role in the history of Islam and the Muslim world. It has been a source of spiritual renewal and inspiration for countless generations of Muslims, and it has also been a powerful force for unity and solidarity among the faithful.\n",
    "\n",
    "# Throughout history, Hajj\n",
    "# 297.3512797355652\n",
    "\n",
    "# <|system|>\n",
    "#  You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n",
    "# <|user|>\n",
    "# what is the origin of hajj?\n",
    "# <|assistant|>\n",
    "# The origin of Hajj can be traced back to the time of Prophet Muhammad (peace be upon him) in the 7th century CE. According to Islamic tradition, Hajj was first performed by Prophet Ibrahim (Abraham) and his son Prophet Ismail (Ishmael) as an act of submission to Allah (God).\n",
    "\n",
    "# However, the annual pilgrimage to the holy city of Mecca, which is an integral part of Hajj, was instituted by Prophet Muhammad during his lifetime. The Prophet's teachings and practices related to Hajj have been preserved and followed by Muslims ever since.\n",
    "\n",
    "# The purpose of Hajj, as stated in the Quran, is to commemorate the unity and brotherhood of the human race, and to affirm the oneness of Allah. It is also a time for Muslims to seek forgiveness, make supplications, and renew their faith and commitment to Allah.\n",
    "\n",
    "# In summary, the origin of Hajj can be traced back to the time of Prophet Ibrahim, but its current form and significance are rooted in the teachings and practices of Prophet Muhammad.\n",
    "# 295.48454308509827\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_memory_footprint()//1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(torch.cuda.current_device())\n",
    "# additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "print_gpu_utilization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMTesting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
