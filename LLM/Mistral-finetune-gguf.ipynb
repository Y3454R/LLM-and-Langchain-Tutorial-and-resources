{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zephyr_base_model = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "zephyr_output_dir = 'KUETLLM_zephyr'\n",
    "zephyr_output_dir_base = 'KUETLLM_zephyr_base'\n",
    "mistral_base_model = 'mistralai/Mistral-7B-v0.1'\n",
    "mistral_output_dir = 'KUETLLM_mistral'\n",
    "mistral_output_dir_base = 'KUETLLM_mistral_base'\n",
    "zephyr_gptq = 'TheBloke/zephyr-7B-beta-GPTQ'\n",
    "mistral_gptq = 'TheBloke/Mistral-7B-v0.1-GPTQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "# login(\"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\") #arbit\n",
    "login(\"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\") #shahidul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"arbitropy/kuetdata\", split=\"train\")\n",
    "data_df = data.to_pandas()\n",
    "data_df[\"text\"] = data_df[[\"Question\", \"Answer\"]].apply(lambda x: \"###Human: \" + str(x[\"Question\"]) + \"\\n###Assistant: \" + str(x[\"Answer\"]), axis=1)\n",
    "print(data_df.iloc[0])\n",
    "dataset = Dataset.from_pandas(data_df)\n",
    "tokenizer = AutoTokenizer.from_pretrained(zephyr_base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
      "Fetching 32 files:   3%|▊                        | 1/32 [00:00<00:12,  2.49it/s]downloading https://huggingface.co/HuggingFaceH4/zephyr-7b-beta/resolve/8d44a71892fca587e7e924fb95d54c0174e14c86/pytorch_model-00002-of-00008.bin to /home/rtx3090/.cache/huggingface/hub/tmpcu7m0j9s\n",
      "Fetching 32 files:   6%|█▌                       | 2/32 [00:00<00:07,  4.03it/s]\n",
      "pytorch_model-00002-of-00008.bin:   0%|             | 0.00/1.95G [00:00<?, ?B/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:   1%|    | 10.5M/1.95G [00:01<03:23, 9.53MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:   1%|    | 21.0M/1.95G [00:01<02:37, 12.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:   2%|    | 31.5M/1.95G [00:02<02:18, 13.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:   2%|    | 41.9M/1.95G [00:03<02:16, 13.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:   3%|    | 52.4M/1.95G [00:03<02:09, 14.6MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:   3%|▏   | 62.9M/1.95G [00:04<02:08, 14.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:   4%|▏   | 73.4M/1.95G [00:05<02:06, 14.8MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:   4%|▏   | 83.9M/1.95G [00:06<02:11, 14.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:   5%|▏   | 94.4M/1.95G [00:06<02:06, 14.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:   5%|▎    | 105M/1.95G [00:07<02:02, 15.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:   6%|▎    | 115M/1.95G [00:08<02:00, 15.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:   6%|▎    | 126M/1.95G [00:08<01:58, 15.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:   7%|▎    | 136M/1.95G [00:09<02:01, 14.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:   8%|▍    | 147M/1.95G [00:10<01:59, 15.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:   8%|▍    | 157M/1.95G [00:10<02:01, 14.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:   9%|▍    | 168M/1.95G [00:11<02:02, 14.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:   9%|▍    | 178M/1.95G [00:12<02:14, 13.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  10%|▍    | 189M/1.95G [00:13<02:12, 13.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  10%|▌    | 199M/1.95G [00:14<02:08, 13.6MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  11%|▌    | 210M/1.95G [00:14<02:01, 14.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  11%|▌    | 220M/1.95G [00:15<02:00, 14.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  12%|▌    | 231M/1.95G [00:16<02:09, 13.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  12%|▌    | 241M/1.95G [00:17<02:42, 10.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  13%|▋    | 252M/1.95G [00:18<02:24, 11.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  13%|▋    | 262M/1.95G [00:19<02:14, 12.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  14%|▋    | 273M/1.95G [00:19<02:07, 13.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  15%|▋    | 283M/1.95G [00:20<01:59, 13.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  15%|▊    | 294M/1.95G [00:21<01:54, 14.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  16%|▊    | 304M/1.95G [00:22<01:56, 14.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  16%|▊    | 315M/1.95G [00:22<01:53, 14.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  17%|▊    | 325M/1.95G [00:23<01:49, 14.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  17%|▊    | 336M/1.95G [00:24<01:48, 14.8MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  18%|▉    | 346M/1.95G [00:24<01:49, 14.6MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  18%|▉    | 357M/1.95G [00:25<01:45, 15.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  19%|▉    | 367M/1.95G [00:26<01:49, 14.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  19%|▉    | 377M/1.95G [00:26<01:45, 14.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  20%|▉    | 388M/1.95G [00:28<02:03, 12.6MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  20%|█    | 398M/1.95G [00:28<01:54, 13.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  21%|█    | 409M/1.95G [00:29<01:53, 13.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  22%|█    | 419M/1.95G [00:31<02:41, 9.45MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  22%|█    | 430M/1.95G [00:32<02:52, 8.78MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  23%|█▏   | 440M/1.95G [00:34<03:02, 8.25MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  23%|█▏   | 451M/1.95G [00:35<02:42, 9.18MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  24%|█▏   | 461M/1.95G [00:35<02:25, 10.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  24%|█▏   | 472M/1.95G [00:36<02:15, 10.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  25%|█▏   | 482M/1.95G [00:37<02:07, 11.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  25%|█▎   | 493M/1.95G [00:38<02:00, 12.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  26%|█▎   | 503M/1.95G [00:39<02:12, 10.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  26%|█▎   | 514M/1.95G [00:40<02:07, 11.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  27%|█▎   | 524M/1.95G [00:40<01:59, 11.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  27%|█▎   | 535M/1.95G [00:41<01:54, 12.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  28%|█▍   | 545M/1.95G [00:42<02:03, 11.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  29%|█▍   | 556M/1.95G [00:43<01:53, 12.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  29%|█▍   | 566M/1.95G [00:44<01:54, 12.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  30%|█▍   | 577M/1.95G [00:45<01:48, 12.6MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  30%|█▌   | 587M/1.95G [00:45<01:44, 13.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  31%|█▌   | 598M/1.95G [00:46<01:47, 12.6MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  31%|█▌   | 608M/1.95G [00:47<01:46, 12.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  32%|█▌   | 619M/1.95G [00:48<01:41, 13.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  32%|█▌   | 629M/1.95G [00:49<01:37, 13.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  33%|█▋   | 640M/1.95G [00:50<01:55, 11.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  33%|█▋   | 650M/1.95G [00:51<01:47, 12.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  34%|█▋   | 661M/1.95G [00:51<01:41, 12.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  34%|█▋   | 671M/1.95G [00:52<01:40, 12.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  35%|█▊   | 682M/1.95G [00:53<01:35, 13.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  36%|█▊   | 692M/1.95G [00:54<01:32, 13.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  36%|█▊   | 703M/1.95G [00:54<01:31, 13.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  37%|█▊   | 713M/1.95G [00:55<01:29, 13.8MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  37%|█▊   | 724M/1.95G [00:56<01:27, 14.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  38%|█▉   | 734M/1.95G [00:57<01:28, 13.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  38%|█▉   | 744M/1.95G [00:57<01:27, 13.8MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  39%|█▉   | 755M/1.95G [00:58<01:25, 13.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  39%|█▉   | 765M/1.95G [00:59<01:27, 13.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  40%|█▉   | 776M/1.95G [01:00<01:25, 13.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  40%|██   | 786M/1.95G [01:01<01:38, 11.8MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  41%|██   | 797M/1.95G [01:02<01:32, 12.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  41%|██   | 807M/1.95G [01:03<01:35, 11.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  42%|██   | 818M/1.95G [01:03<01:32, 12.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  43%|██▏  | 828M/1.95G [01:04<01:27, 12.8MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  43%|██▏  | 839M/1.95G [01:05<01:23, 13.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  44%|██▏  | 849M/1.95G [01:06<01:20, 13.6MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  44%|██▏  | 860M/1.95G [01:06<01:21, 13.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  45%|██▏  | 870M/1.95G [01:07<01:18, 13.6MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  45%|██▎  | 881M/1.95G [01:08<01:17, 13.8MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  46%|██▎  | 891M/1.95G [01:09<01:18, 13.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  46%|██▎  | 902M/1.95G [01:09<01:15, 13.8MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  47%|██▎  | 912M/1.95G [01:11<01:26, 12.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  47%|██▎  | 923M/1.95G [01:12<01:30, 11.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  48%|██▍  | 933M/1.95G [01:13<01:34, 10.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  48%|██▍  | 944M/1.95G [01:14<01:29, 11.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  49%|██▍  | 954M/1.95G [01:14<01:29, 11.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  50%|██▍  | 965M/1.95G [01:15<01:21, 12.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  50%|██▌  | 975M/1.95G [01:16<01:18, 12.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  51%|██▌  | 986M/1.95G [01:17<01:13, 13.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  51%|██▌  | 996M/1.95G [01:18<01:15, 12.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  52%|██  | 1.01G/1.95G [01:18<01:13, 12.8MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  52%|██  | 1.02G/1.95G [01:20<01:29, 10.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  53%|██  | 1.03G/1.95G [01:21<01:21, 11.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  53%|██▏ | 1.04G/1.95G [01:21<01:17, 11.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  54%|██▏ | 1.05G/1.95G [01:22<01:18, 11.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  54%|██▏ | 1.06G/1.95G [01:24<01:26, 10.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  55%|██▏ | 1.07G/1.95G [01:24<01:17, 11.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  55%|██▏ | 1.08G/1.95G [01:25<01:11, 12.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  56%|██▏ | 1.09G/1.95G [01:26<01:07, 12.8MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  57%|██▎ | 1.10G/1.95G [01:27<01:03, 13.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  57%|██▎ | 1.11G/1.95G [01:27<01:01, 13.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  58%|██▎ | 1.12G/1.95G [01:28<00:59, 13.8MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  58%|██▎ | 1.13G/1.95G [01:29<00:58, 14.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  59%|██▎ | 1.14G/1.95G [01:29<00:57, 14.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  59%|██▎ | 1.15G/1.95G [01:30<00:55, 14.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  60%|██▍ | 1.16G/1.95G [01:31<00:55, 14.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  60%|██▍ | 1.17G/1.95G [01:32<00:55, 13.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  61%|██▍ | 1.18G/1.95G [01:32<00:53, 14.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  61%|██▍ | 1.20G/1.95G [01:33<00:52, 14.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  62%|██▍ | 1.21G/1.95G [01:34<00:51, 14.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  62%|██▍ | 1.22G/1.95G [01:35<00:50, 14.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  63%|██▌ | 1.23G/1.95G [01:35<00:49, 14.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  64%|██▌ | 1.24G/1.95G [01:36<00:49, 14.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  64%|██▌ | 1.25G/1.95G [01:37<00:47, 14.6MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  65%|██▌ | 1.26G/1.95G [01:37<00:47, 14.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  65%|██▌ | 1.27G/1.95G [01:38<00:46, 14.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  66%|██▋ | 1.28G/1.95G [01:39<00:46, 14.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  66%|██▋ | 1.29G/1.95G [01:40<00:45, 14.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  67%|██▋ | 1.30G/1.95G [01:40<00:44, 14.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  67%|██▋ | 1.31G/1.95G [01:41<00:43, 14.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  68%|██▋ | 1.32G/1.95G [01:42<00:45, 13.8MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  68%|██▋ | 1.33G/1.95G [01:43<00:43, 14.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  69%|██▊ | 1.34G/1.95G [01:43<00:42, 14.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  70%|██▊ | 1.35G/1.95G [01:44<00:41, 14.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  70%|██▊ | 1.36G/1.95G [01:45<00:40, 14.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  71%|██▊ | 1.37G/1.95G [01:46<00:39, 14.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  71%|██▊ | 1.38G/1.95G [01:46<00:40, 14.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  72%|██▊ | 1.39G/1.95G [01:47<00:38, 14.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  72%|██▉ | 1.41G/1.95G [01:48<00:37, 14.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  73%|██▉ | 1.42G/1.95G [01:48<00:37, 14.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  73%|██▉ | 1.43G/1.95G [01:49<00:36, 14.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  74%|██▉ | 1.44G/1.95G [01:50<00:35, 14.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  74%|██▉ | 1.45G/1.95G [01:51<00:34, 14.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  75%|██▉ | 1.46G/1.95G [01:51<00:33, 14.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  75%|███ | 1.47G/1.95G [01:53<00:40, 11.8MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  76%|███ | 1.48G/1.95G [01:53<00:38, 12.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  77%|███ | 1.49G/1.95G [01:54<00:35, 12.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  77%|███ | 1.50G/1.95G [01:55<00:33, 13.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  78%|███ | 1.51G/1.95G [01:56<00:32, 13.6MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  78%|███ | 1.52G/1.95G [01:56<00:30, 13.8MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  79%|███▏| 1.53G/1.95G [01:57<00:29, 13.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  79%|███▏| 1.54G/1.95G [01:58<00:29, 13.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  80%|███▏| 1.55G/1.95G [01:59<00:27, 14.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  80%|███▏| 1.56G/1.95G [01:59<00:27, 14.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  81%|███▏| 1.57G/1.95G [02:00<00:26, 14.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  81%|███▎| 1.58G/1.95G [02:01<00:26, 13.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  82%|███▎| 1.59G/1.95G [02:02<00:25, 13.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  82%|███▎| 1.60G/1.95G [02:02<00:26, 13.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  83%|███▎| 1.61G/1.95G [02:03<00:24, 13.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  84%|███▎| 1.63G/1.95G [02:04<00:25, 12.6MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  84%|███▎| 1.64G/1.95G [02:05<00:23, 13.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  85%|███▍| 1.65G/1.95G [02:06<00:22, 13.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  85%|███▍| 1.66G/1.95G [02:07<00:24, 11.8MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  86%|███▍| 1.67G/1.95G [02:08<00:22, 12.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  86%|███▍| 1.68G/1.95G [02:08<00:20, 13.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  87%|███▍| 1.69G/1.95G [02:09<00:19, 13.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  87%|███▍| 1.70G/1.95G [02:10<00:18, 13.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  88%|███▌| 1.71G/1.95G [02:10<00:16, 14.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  88%|███▌| 1.72G/1.95G [02:11<00:16, 13.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  89%|███▌| 1.73G/1.95G [02:12<00:15, 13.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  89%|███▌| 1.74G/1.95G [02:13<00:14, 14.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  90%|███▌| 1.75G/1.95G [02:13<00:13, 14.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  91%|███▌| 1.76G/1.95G [02:14<00:12, 14.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  91%|███▋| 1.77G/1.95G [02:15<00:12, 13.8MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  92%|███▋| 1.78G/1.95G [02:16<00:11, 14.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  92%|███▋| 1.79G/1.95G [02:16<00:10, 14.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  93%|███▋| 1.80G/1.95G [02:17<00:10, 14.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  93%|███▋| 1.81G/1.95G [02:18<00:11, 11.9MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  94%|███▋| 1.82G/1.95G [02:19<00:09, 12.5MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  94%|███▊| 1.84G/1.95G [02:20<00:08, 13.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  95%|███▊| 1.85G/1.95G [02:21<00:07, 13.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  95%|███▊| 1.86G/1.95G [02:21<00:06, 13.7MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  96%|███▊| 1.87G/1.95G [02:22<00:05, 14.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  96%|███▊| 1.88G/1.95G [02:23<00:04, 14.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  97%|███▉| 1.89G/1.95G [02:23<00:04, 14.2MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  98%|███▉| 1.90G/1.95G [02:24<00:03, 14.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  98%|███▉| 1.91G/1.95G [02:25<00:02, 14.3MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  99%|███▉| 1.92G/1.95G [02:26<00:01, 14.4MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin:  99%|███▉| 1.93G/1.95G [02:26<00:01, 14.0MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin: 100%|███▉| 1.94G/1.95G [02:27<00:00, 14.1MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00008.bin: 100%|████| 1.95G/1.95G [02:28<00:00, 13.1MB/s]\u001b[A\n",
      "Fetching 32 files: 100%|████████████████████████| 32/32 [02:29<00:00,  4.66s/it]\n",
      "/home/rtx3090/notebooks/base_model_full\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli download HuggingFaceH4/zephyr-7b-beta --local-dir base_model_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file base_model_full/model-00001-of-00008.safetensors\n",
      "Loading model file base_model_full/model-00001-of-00008.safetensors\n",
      "Loading model file base_model_full/model-00002-of-00008.safetensors\n",
      "Loading model file base_model_full/model-00003-of-00008.safetensors\n",
      "Loading model file base_model_full/model-00004-of-00008.safetensors\n",
      "Loading model file base_model_full/model-00005-of-00008.safetensors\n",
      "Loading model file base_model_full/model-00006-of-00008.safetensors\n",
      "Loading model file base_model_full/model-00007-of-00008.safetensors\n",
      "Loading model file base_model_full/model-00008-of-00008.safetensors\n",
      "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=32768, n_ff=14336, n_head=32, n_head_kv=8, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('base_model_full'))\n",
      "Loading vocab file 'base_model_full/tokenizer.model', type 'spm'\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "Permuting layer 22\n",
      "Permuting layer 23\n",
      "Permuting layer 24\n",
      "Permuting layer 25\n",
      "Permuting layer 26\n",
      "Permuting layer 27\n",
      "Permuting layer 28\n",
      "Permuting layer 29\n",
      "Permuting layer 30\n",
      "Permuting layer 31\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | BF16   | [32000, 4096]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "lm_head.weight                                   -> output.weight                            | BF16   | [32000, 4096]\n",
      "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.norm.weight                                -> output_norm.weight                       | BF16   | [4096]\n",
      "Writing base_model_full/ggml-model-f16.gguf, format 1\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting special token type pad to 2\n",
      "gguf: Setting chat_template to {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   0\n",
      "[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   0\n",
      "[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   0\n",
      "[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   0\n",
      "[  5/291] Writing tensor blk.0.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   0\n",
      "[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   0\n",
      "[  7/291] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   0\n",
      "[  8/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   0\n",
      "[  9/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0\n",
      "[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   0\n",
      "[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   0\n",
      "[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   0\n",
      "[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   1\n",
      "[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   1\n",
      "[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   1\n",
      "[ 17/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   1\n",
      "[ 20/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 21/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   1\n",
      "[ 22/291] Writing tensor blk.2.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   1\n",
      "[ 23/291] Writing tensor blk.2.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   1\n",
      "[ 24/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 25/291] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   1\n",
      "[ 26/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 27/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 28/291] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   1\n",
      "[ 29/291] Writing tensor blk.3.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   1\n",
      "[ 30/291] Writing tensor blk.3.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   1\n",
      "[ 31/291] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   1\n",
      "[ 32/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 33/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1\n",
      "[ 34/291] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   1\n",
      "[ 35/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 36/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   1\n",
      "[ 37/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+   1\n",
      "[ 38/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
      "[ 39/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   1\n",
      "[ 40/291] Writing tensor blk.4.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   1\n",
      "[ 41/291] Writing tensor blk.4.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   1\n",
      "[ 42/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 43/291] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
      "[ 44/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 45/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 46/291] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
      "[ 47/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 48/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   2\n",
      "[ 49/291] Writing tensor blk.5.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   2\n",
      "[ 50/291] Writing tensor blk.5.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   2\n",
      "[ 51/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 52/291] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
      "[ 53/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 54/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 55/291] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
      "[ 56/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 57/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   2\n",
      "[ 58/291] Writing tensor blk.6.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   2\n",
      "[ 59/291] Writing tensor blk.6.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   2\n",
      "[ 60/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 61/291] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
      "[ 62/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 63/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 64/291] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
      "[ 65/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 66/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   2\n",
      "[ 67/291] Writing tensor blk.7.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   2\n",
      "[ 68/291] Writing tensor blk.7.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   2\n",
      "[ 69/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[ 70/291] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
      "[ 71/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 72/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 73/291] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   3\n",
      "[ 74/291] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   3\n",
      "[ 75/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 76/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 77/291] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   3\n",
      "[ 78/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+   3\n",
      "[ 79/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   3\n",
      "[ 80/291] Writing tensor blk.10.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 81/291] Writing tensor blk.10.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 82/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[ 83/291] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   3\n",
      "[ 84/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 85/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 86/291] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   3\n",
      "[ 87/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+   3\n",
      "[ 88/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   3\n",
      "[ 89/291] Writing tensor blk.11.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 90/291] Writing tensor blk.11.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 91/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[ 92/291] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   3\n",
      "[ 93/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 94/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 95/291] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   3\n",
      "[ 96/291] Writing tensor blk.12.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 97/291] Writing tensor blk.12.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 98/291] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   4\n",
      "[ 99/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[100/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[101/291] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   4\n",
      "[102/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[103/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   4\n",
      "[104/291] Writing tensor blk.8.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   4\n",
      "[105/291] Writing tensor blk.8.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   4\n",
      "[106/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+   4\n",
      "[107/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[108/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   4\n",
      "[109/291] Writing tensor blk.9.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   4\n",
      "[110/291] Writing tensor blk.9.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   4\n",
      "[111/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+   4\n",
      "[112/291] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   4\n",
      "[113/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+   4\n",
      "[114/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   4\n",
      "[115/291] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   4\n",
      "[116/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[117/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   4\n",
      "[118/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[119/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[120/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   4\n",
      "[121/291] Writing tensor blk.13.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   4\n",
      "[122/291] Writing tensor blk.13.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   4\n",
      "[123/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[124/291] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   4\n",
      "[125/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4\n",
      "[126/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4\n",
      "[127/291] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   4\n",
      "[128/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+   4\n",
      "[129/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   5\n",
      "[130/291] Writing tensor blk.14.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   5\n",
      "[131/291] Writing tensor blk.14.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   5\n",
      "[132/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[133/291] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   5\n",
      "[134/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[135/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[136/291] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   5\n",
      "[137/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[138/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   5\n",
      "[139/291] Writing tensor blk.15.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   5\n",
      "[140/291] Writing tensor blk.15.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   5\n",
      "[141/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[142/291] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   5\n",
      "[143/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[144/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[145/291] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   5\n",
      "[146/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[147/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   5\n",
      "[148/291] Writing tensor blk.16.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   5\n",
      "[149/291] Writing tensor blk.16.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   5\n",
      "[150/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[151/291] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   5\n",
      "[152/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[153/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[154/291] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   5\n",
      "[155/291] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   5\n",
      "[156/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5\n",
      "[157/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5\n",
      "[158/291] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   5\n",
      "[159/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[160/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   6\n",
      "[161/291] Writing tensor blk.17.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   6\n",
      "[162/291] Writing tensor blk.17.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   6\n",
      "[163/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[164/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[165/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   6\n",
      "[166/291] Writing tensor blk.18.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   6\n",
      "[167/291] Writing tensor blk.18.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   6\n",
      "[168/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[169/291] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   6\n",
      "[170/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[171/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[172/291] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   6\n",
      "[173/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[174/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   6\n",
      "[175/291] Writing tensor blk.19.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   6\n",
      "[176/291] Writing tensor blk.19.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   6\n",
      "[177/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[178/291] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   6\n",
      "[179/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[180/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[181/291] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   6\n",
      "[182/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[183/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   7\n",
      "[184/291] Writing tensor blk.20.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   7\n",
      "[185/291] Writing tensor blk.20.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   7\n",
      "[186/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[187/291] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   7\n",
      "[188/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[189/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[190/291] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   7\n",
      "[191/291] Writing tensor blk.21.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   7\n",
      "[192/291] Writing tensor blk.21.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   7\n",
      "[193/291] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   7\n",
      "[194/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[195/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[196/291] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   7\n",
      "[197/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[198/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   7\n",
      "[199/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[200/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[201/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   7\n",
      "[202/291] Writing tensor blk.22.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   7\n",
      "[203/291] Writing tensor blk.22.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   7\n",
      "[204/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[205/291] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   7\n",
      "[206/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[207/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[208/291] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   7\n",
      "[209/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[210/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   8\n",
      "[211/291] Writing tensor blk.23.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   8\n",
      "[212/291] Writing tensor blk.23.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   8\n",
      "[213/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[214/291] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   8\n",
      "[215/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[216/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[217/291] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   8\n",
      "[218/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+   8\n",
      "[219/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   8\n",
      "[220/291] Writing tensor blk.24.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   8\n",
      "[221/291] Writing tensor blk.24.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   8\n",
      "[222/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[223/291] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   8\n",
      "[224/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[225/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[226/291] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   8\n",
      "[227/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+   8\n",
      "[228/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   8\n",
      "[229/291] Writing tensor blk.25.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   8\n",
      "[230/291] Writing tensor blk.25.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   8\n",
      "[231/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[232/291] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   8\n",
      "[233/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[234/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[235/291] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   8\n",
      "[236/291] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   8\n",
      "[237/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[238/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[239/291] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   8\n",
      "[240/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+   8\n",
      "[241/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   9\n",
      "[242/291] Writing tensor blk.26.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   9\n",
      "[243/291] Writing tensor blk.26.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   9\n",
      "[244/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+   9\n",
      "[245/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+   9\n",
      "[246/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   9\n",
      "[247/291] Writing tensor blk.27.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   9\n",
      "[248/291] Writing tensor blk.27.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   9\n",
      "[249/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+   9\n",
      "[250/291] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   9\n",
      "[251/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9\n",
      "[252/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[253/291] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   9\n",
      "[254/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+   9\n",
      "[255/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   9\n",
      "[256/291] Writing tensor blk.28.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   9\n",
      "[257/291] Writing tensor blk.28.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   9\n",
      "[258/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+   9\n",
      "[259/291] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   9\n",
      "[260/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9\n",
      "[261/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[262/291] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   9\n",
      "[263/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+   9\n",
      "[264/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  10\n",
      "[265/291] Writing tensor blk.29.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  10\n",
      "[266/291] Writing tensor blk.29.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  10\n",
      "[267/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  10\n",
      "[268/291] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  10\n",
      "[269/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+  10\n",
      "[270/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  10\n",
      "[271/291] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  10\n",
      "[272/291] Writing tensor blk.30.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  10\n",
      "[273/291] Writing tensor blk.30.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  10\n",
      "[274/291] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  10\n",
      "[275/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+  10\n",
      "[276/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  10\n",
      "[277/291] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  10\n",
      "[278/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+  10\n",
      "[279/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+  11\n",
      "[280/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  11\n",
      "[281/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+  11\n",
      "[282/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+  11\n",
      "[283/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  11\n",
      "[284/291] Writing tensor blk.31.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  11\n",
      "[285/291] Writing tensor blk.31.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  11\n",
      "[286/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  11\n",
      "[287/291] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  11\n",
      "[288/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+  11\n",
      "[289/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  11\n",
      "[290/291] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  11\n",
      "[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+  11\n",
      "Wrote base_model_full/ggml-model-f16.gguf\n"
     ]
    }
   ],
   "source": [
    "! python ./llama.cpp/convert.py ./base_model_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ae7ad8075f46c5bac6607d85642507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = zephyr_base_model\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)\n",
    "# loading the model with quantization config\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "merged_model = PeftModel.from_pretrained(base_model, zephyr_output_dir)\n",
    "merged_model = merged_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_model.push_to_hub(zephyr_output_dir_base, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(zephyr_output_dir_base, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file base_model/model-00001-of-00002.safetensors\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rtx3090/notebooks/./llama.cpp/convert.py\", line 1228, in <module>\n",
      "    main()\n",
      "  File \"/home/rtx3090/notebooks/./llama.cpp/convert.py\", line 1161, in main\n",
      "    model_plus = load_some_model(args.model)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rtx3090/notebooks/./llama.cpp/convert.py\", line 1076, in load_some_model\n",
      "    models_plus.append(lazy_load_file(path))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rtx3090/notebooks/./llama.cpp/convert.py\", line 753, in lazy_load_file\n",
      "    return lazy_load_safetensors_file(fp, path)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rtx3090/notebooks/./llama.cpp/convert.py\", line 732, in lazy_load_safetensors_file\n",
      "    model = {name: convert(info) for (name, info) in header.items() if name != '__metadata__'}\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rtx3090/notebooks/./llama.cpp/convert.py\", line 732, in <dictcomp>\n",
      "    model = {name: convert(info) for (name, info) in header.items() if name != '__metadata__'}\n",
      "                   ^^^^^^^^^^^^^\n",
      "  File \"/home/rtx3090/notebooks/./llama.cpp/convert.py\", line 720, in convert\n",
      "    data_type = SAFETENSORS_DATA_TYPES[info['dtype']]\n",
      "                ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^\n",
      "KeyError: 'I8'\n"
     ]
    }
   ],
   "source": [
    "! python ./llama.cpp/convert.py ./base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rtx3090/notebooks/./llama.cpp/convert-lora-to-ggml.py\", line 92, in <module>\n",
      "    model = torch.load(input_model, map_location=\"cpu\")\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rtx3090/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/torch/serialization.py\", line 986, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rtx3090/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/torch/serialization.py\", line 435, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rtx3090/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/torch/serialization.py\", line 416, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "                     ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './KUETLLM_zephyr/adapter_model.bin'\n"
     ]
    }
   ],
   "source": [
    "! python ./llama.cpp/convert-lora-to-ggml.py ./KUETLLM_zephyr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMTesting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
