{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from trl import SFTTrainer\n",
    "import transformers\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import GenerationConfig\n",
    "from pynvml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "lora_output = 'KUETLLM_zephyr7b_lora'\n",
    "full_output = 'KUETLLM_zephyr7b_beta'\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "# login(\"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\") #arbit\n",
    "login(\"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\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### huggingface dataset with Prompt, Answer pair\n",
    "\n",
    "# data = load_dataset(\"huggingface/repo\", split=\"train\")\n",
    "# data_df = data.to_pandas()\n",
    "\n",
    "### read csv with Prompt, Answer pair \n",
    "data_location = r\"Data/Hajj.csv\"\n",
    "data_df=pd.read_csv( data_location ,encoding='unicode_escape')\n",
    "\n",
    "### formatting function using tokenizer chat template, system text is set for KUETLLM\n",
    "def formatted_text(x):\n",
    "    temp = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a KUET authority managed chatbot, help users by answering their queries about KUET.\"},\n",
    "    {\"role\": \"user\", \"content\": x[\"Prompt\"]},\n",
    "    {\"role\": \"assistant\", \"content\": x[\"Answer\"]}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(temp, add_generation_prompt=False, tokenize=False)\n",
    "\n",
    "### set formatting\n",
    "data_df[\"text\"] = data_df[[\"Prompt\", \"Answer\"]].apply(lambda x: formatted_text(x), axis=1)\n",
    "print(data_df.iloc[0])\n",
    "dataset = Dataset.from_pandas(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get quantized model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                                          load_in_8bit=True,     # call for the 8 bit bnb quantized version\n",
    "                                                          device_map='auto'\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print model to find lora layers\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PEFT adapter config (16:32)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# target modules are currently selected for zephyr base model\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\",\"k_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],   # target all the linear layers for full finetuning\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stabilize output layer and layernorms\n",
    "model = prepare_model_for_kbit_training(model, 8)\n",
    "# Set PEFT adapter on model (Last step)\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters\n",
    "MAXLEN=512\n",
    "BATCH_SIZE=4\n",
    "GRAD_ACC=4\n",
    "OPTIMIZER='paged_adamw_8bit' # save memory\n",
    "LR=5e-06                      # slightly smaller than pretraining lr | and close to LoRA standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training config\n",
    "training_config = transformers.TrainingArguments(per_device_train_batch_size=BATCH_SIZE,\n",
    "                                                 gradient_accumulation_steps=GRAD_ACC,\n",
    "                                                 optim=OPTIMIZER,\n",
    "                                                 learning_rate=LR,\n",
    "                                                 fp16=True,            # consider compatibility when using bf16\n",
    "                                                 logging_steps=10,\n",
    "                                                 num_train_epochs = 2,\n",
    "                                                 output_dir=lora_output,\n",
    "                                                 remove_unused_columns=False,\n",
    "                                                 )\n",
    "\n",
    "# Set collator\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "# Setup trainer\n",
    "trainer = SFTTrainer(model=model,\n",
    "                               train_dataset=dataset,\n",
    "                               data_collator=data_collator,\n",
    "                               args=training_config,\n",
    "                               dataset_text_field=\"text\",\n",
    "                            #    callbacks=[early_stop], need to learn, lora easily overfits\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(lora_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at 'HajjLLM_zephyr7b_lora'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:270\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    271\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/HajjLLM_zephyr7b_lora/resolve/main/adapter_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/peft/config.py:105\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     config_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    106\u001b[0m         pretrained_model_name_or_path, CONFIG_NAME, subfolder\u001b[39m=\u001b[39;49msubfolder, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhf_hub_download_kwargs\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/huggingface_hub/file_download.py:1374\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1373\u001b[0m     \u001b[39m# Repo not found => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1374\u001b[0m     \u001b[39mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[39m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/huggingface_hub/file_download.py:1247\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1247\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1248\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1249\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1250\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1251\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1252\u001b[0m     )\n\u001b[1;32m   1253\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1254\u001b[0m     \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/huggingface_hub/file_download.py:1624\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1623\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1624\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1625\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1626\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1627\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1628\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1629\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1630\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1631\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1632\u001b[0m )\n\u001b[1;32m   1633\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/huggingface_hub/file_download.py:402\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 402\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    403\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    404\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    405\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    406\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    407\u001b[0m     )\n\u001b[1;32m    409\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/huggingface_hub/file_download.py:426\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    425\u001b[0m response \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m--> 426\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    427\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:320\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    312\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    313\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m make sure you are authenticated.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    319\u001b[0m     )\n\u001b[0;32m--> 320\u001b[0m     \u001b[39mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-6571eab0-24d55dd154c296b408cf5f35;25ed2d63-6aed-4cd7-96f7-ef2ebd2333a9)\n\nRepository Not Found for url: https://huggingface.co/HajjLLM_zephyr7b_lora/resolve/main/adapter_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/rtx3090/notebooks/finetune-llm-lora.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rtx3090/notebooks/finetune-llm-lora.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Get peft config\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rtx3090/notebooks/finetune-llm-lora.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpeft\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftConfig\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rtx3090/notebooks/finetune-llm-lora.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m config \u001b[39m=\u001b[39m PeftConfig\u001b[39m.\u001b[39;49mfrom_pretrained(lora_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLMTesting/lib/python3.11/site-packages/peft/config.py:109\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m         config_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    106\u001b[0m             pretrained_model_name_or_path, CONFIG_NAME, subfolder\u001b[39m=\u001b[39msubfolder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhf_hub_download_kwargs\n\u001b[1;32m    107\u001b[0m         )\n\u001b[1;32m    108\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mCONFIG_NAME\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m at \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m loaded_attributes \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_json_file(config_file)\n\u001b[1;32m    113\u001b[0m \u001b[39m# TODO: this hack is needed to fix the following issue (on commit 702f937):\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39m# if someone saves a default config and loads it back with `PeftConfig` class it yields to\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39m# not loading the correct config class.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39m# print(peft_config)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39m# >>> PeftConfig(peft_type='ADALORA', auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 'HajjLLM_zephyr7b_lora'"
     ]
    }
   ],
   "source": [
    "# Get peft config\n",
    "from peft import PeftConfig\n",
    "config = PeftConfig.from_pretrained(lora_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef972ebc1c1b4c5a81b170b511c43a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get base model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
    "                                                          return_dict=True,\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('zephyr-7b-beta-base-full/tokenizer_config.json',\n",
       " 'zephyr-7b-beta-base-full/special_tokens_map.json',\n",
       " 'zephyr-7b-beta-base-full/tokenizer.model',\n",
       " 'zephyr-7b-beta-base-full/added_tokens.json',\n",
       " 'zephyr-7b-beta-base-full/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(base_model,\n",
    "                                                       add_eos_token=True\n",
    "                                                       )\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.save_pretrained(\"zephyr-7b-beta-base-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Lora model\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, lora_output)\n",
    "\n",
    "# Get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(config.base_model_name_or_path,\n",
    "                                                       add_eos_token=True\n",
    "                                                       )\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(full_output)\n",
    "tokenizer.save_pretrained(full_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push model to hub\n",
    "merged_model.push_to_hub(full_output)\n",
    "tokenizer.push_to_hub(full_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load for inferencing\n",
    "tokenizer = AutoTokenizer.from_pretrained(full_output)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(full_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161f8dc236be4405a20ae32131838855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load base for comparison\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('zephyr7b-beta-full/tokenizer_config.json',\n",
       " 'zephyr7b-beta-full/special_tokens_map.json',\n",
       " 'zephyr7b-beta-full/tokenizer.model',\n",
       " 'zephyr7b-beta-full/added_tokens.json',\n",
       " 'zephyr7b-beta-full/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"zephyr7b-beta-full\")\n",
    "tokenizer.save_pretrained(\"zephyr7b-beta-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### inferencing on the new model\n",
    "\n",
    "def process_data_sample(example):\n",
    "\n",
    "    processed_example = \"<|system|>\\n You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\\n<|user|>\\n\" + example[\"instruction\"] + \"\\n<|assistant|>\\n\"\n",
    "\n",
    "    return processed_example\n",
    "\n",
    "inp_str = process_data_sample(\n",
    "    {\n",
    "        # \"instruction\": \"Tell me about the importance of Tawaf.\",\n",
    "        # \"instruction\": \"What are the steps of Tawaf?\",\n",
    "        # \"instruction\": \"What are the steps of Hajj?\",\n",
    "        # \"instruction\": \"What happens in Mina during Hajj?\",\n",
    "        \"instruction\": \"What should I do after I reach mina while in Hajj?\",\n",
    "        # \"instruction\": \"Tell me about the history of Hajj.\",\n",
    "        # \"instruction\": \"What started Hajj historically?\",\n",
    "        # \"instruction\": \"what is the origin of hajj?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "inputs = tokenizer(inp_str, return_tensors=\"pt\")\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    top_k=1,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "import time\n",
    "st_time = time.time()\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(time.time()-st_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetuned:\n",
    "# <|system|>\n",
    "#  You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n",
    "# <|user|>\n",
    "# What started Hajj historically?\n",
    "# <|assistant|>\n",
    "# Hajj started as a pilgrimage to the Kaaba, which was a sacred site for the ancient Arab tribes. The Kaaba was believed to be the house of Allah, and the pilgrimage was a way to honor and worship Him. The practice of Hajj became more formalized and structured over time, with specific rituals and traditions developing. Today, Hajj is a significant religious event for Muslims around the world.\n",
    "# 108.65042996406555\n",
    "\n",
    "# <|system|>\n",
    "#  You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n",
    "# <|user|>\n",
    "# Tell me about the history of Hajj.\n",
    "# <|assistant|>\n",
    "# Hajj is one of the five pillars of Islam, and its history dates back to the time of the Prophet Muhammad. The first recorded Hajj was in 632 CE, during the Prophet's lifetime. The rituals of Hajj have evolved over time, with some changes made by the Prophet himself and others added by subsequent generations of Muslims. The Hajj pilgrimage has also played a significant role in Islamic history, serving as a symbol of unity and solidarity among Muslims from different parts of the world. Today, Hajj remains a central part of Islamic practice, attracting millions of pilgrims each year.\n",
    "# 159.5786316394806\n",
    "\n",
    "# <|system|>\n",
    "#  You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n",
    "# <|user|>\n",
    "# what is the origin of hajj?\n",
    "# <|assistant|>\n",
    "# Hajj is a religious pilgrimage to Mecca, Saudi Arabia, that is an essential part of Islamic faith. The practice of hajj dates back to the time of the Prophet Muhammad, who established it as a religious obligation for Muslims. The pilgrimage is a symbolic reenactment of the experiences of the Prophet Muhammad and his wife, Khadijah, during their pilgrimage to Mecca. The practice of hajj has been an integral part of Islamic tradition for over 1,400 years.\n",
    "# 134.81562113761902\n",
    "\n",
    "# Base:\n",
    "# <|system|>\n",
    "#  You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n",
    "# <|user|>\n",
    "# What started Hajj historically?\n",
    "# <|assistant|>\n",
    "# The practice of Hajj, as one of the five pillars of Islam, can be traced back to the time of Prophet Muhammad (peace be upon him) in the 7th century CE. However, the concept of pilgrimage to the holy city of Mecca for worship and devotion can be traced back to pre-Islamic times, as evidenced by historical records and archaeological findings. The ancient Arab tribes used to visit the Kaaba, a cube-shaped structure in the center of the Grand Mosque in Mecca, as a place of worship and pilgrimage. The Prophet Muhammad's mission to unify and spread Islam brought a new dimension to the practice of Hajj, making it a mandatory religious obligation for all able-bodied Muslims who can afford it to perform once in their lifetime.\n",
    "# 208.76106905937195\n",
    "\n",
    "# <|system|>\n",
    "#  You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n",
    "# <|user|>\n",
    "# Tell me about the history of Hajj.\n",
    "# <|assistant|>\n",
    "# Certainly! The practice of Hajj, which is one of the five pillars of Islam, has a rich and fascinating history that dates back over 1,400 years.\n",
    "\n",
    "# The origins of Hajj can be traced back to the time of the Prophet Muhammad, who received the first revelation of the Quran in 610 CE. According to Islamic tradition, the Prophet Muhammad was commanded by God to make a pilgrimage to the holy city of Mecca, which at the time was a pagan center of worship.\n",
    "\n",
    "# The Prophet Muhammad completed his first pilgrimage, known as the Farewell Pilgrimage, in 632 CE, just a few months before his death. This pilgrimage is considered to be the first official Hajj, and it established many of the rituals and traditions that are still observed today.\n",
    "\n",
    "# Over the centuries, Hajj has played a significant role in the history of Islam and the Muslim world. It has been a source of spiritual renewal and inspiration for countless generations of Muslims, and it has also been a powerful force for unity and solidarity among the faithful.\n",
    "\n",
    "# Throughout history, Hajj\n",
    "# 297.3512797355652\n",
    "\n",
    "# <|system|>\n",
    "#  You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n",
    "# <|user|>\n",
    "# what is the origin of hajj?\n",
    "# <|assistant|>\n",
    "# The origin of Hajj can be traced back to the time of Prophet Muhammad (peace be upon him) in the 7th century CE. According to Islamic tradition, Hajj was first performed by Prophet Ibrahim (Abraham) and his son Prophet Ismail (Ishmael) as an act of submission to Allah (God).\n",
    "\n",
    "# However, the annual pilgrimage to the holy city of Mecca, which is an integral part of Hajj, was instituted by Prophet Muhammad during his lifetime. The Prophet's teachings and practices related to Hajj have been preserved and followed by Muslims ever since.\n",
    "\n",
    "# The purpose of Hajj, as stated in the Quran, is to commemorate the unity and brotherhood of the human race, and to affirm the oneness of Allah. It is also a time for Muslims to seek forgiveness, make supplications, and renew their faith and commitment to Allah.\n",
    "\n",
    "# In summary, the origin of Hajj can be traced back to the time of Prophet Ibrahim, but its current form and significance are rooted in the teachings and practices of Prophet Muhammad.\n",
    "# 295.48454308509827\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28649\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()//1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(torch.cuda.current_device())\n",
    "# additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 23473 MB.\n"
     ]
    }
   ],
   "source": [
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "print_gpu_utilization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMTesting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
